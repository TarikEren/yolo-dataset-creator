{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327e5152",
   "metadata": {},
   "source": [
    "## Creating a YOLO Formatted Dataset from Images and YOLO Labels\n",
    "\n",
    "A given YOLO dataset has to have a specific directory tree. It should consist of a root folder usually named `dataset` and sub folders named `train, test, val`\n",
    "\n",
    "The directory tree for a given YOLO dataset looks something like this:\n",
    "<pre>\n",
    ".\n",
    "└── dataset/\n",
    "    ├── images/\n",
    "    │   ├── train\n",
    "    │   ├── test\n",
    "    │   └── val\n",
    "    └── labels/\n",
    "        ├── train\n",
    "        ├── test\n",
    "        └── val\n",
    "</pre>\n",
    "\n",
    "It should be considered that the test directory is optional but recommended for the performance of the model.\n",
    "\n",
    "The following program applies a `80 / 10 / 10` split on the given images and their corresponding labels by default.\n",
    "\n",
    "A `data.yaml` file should be created in order for YOLO to use the dataset.\n",
    "\n",
    "An example `data.yaml` file would look something like this:\n",
    "\n",
    "```\n",
    "path: dataset     # The dataset root\n",
    "\n",
    "train: dataset\\train     # Path to train\n",
    "test: dataset\\test       # Path to test\n",
    "val: dataset\\val         # Path to val\n",
    "\n",
    "names:  # Class names\n",
    "\t0: class0\n",
    "\t1: class1\n",
    "\t2: class2\n",
    "\t3: class3\n",
    "\t4: class4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17766f33",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "The bulk of the entire process is in this section\n",
    "\n",
    "- We need to decide what ratio the train, test and validation splits should have. As stated above, by default the split is `80 / 10 / 10`; train, test and validation sets respectively.\n",
    "\n",
    "- We also need to get the length of all images and labels to get the count of each split.\n",
    "\n",
    "- Lastly, using the list of images and the calculated lengths, we calculate which images belong to which split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ec992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into: \n",
      "    Train: 3426\n",
      "    Val: 428\n",
      "    Test: 428\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Ratios for train, test and validation sets\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.1\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "# List for all supported extensions.\n",
    "SUPPORTED_EXTENSIONS = ['.bmp', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.dng']\n",
    "\n",
    "# Required paths\n",
    "images_path = Path(\"./images\")\n",
    "labels_path = Path(\"./labels\")\n",
    "dataset_path = Path(\"./dataset\")\n",
    "\n",
    "def split_images(train_ratio: float,\n",
    "                 test_ratio: float,\n",
    "                 val_ratio: float,\n",
    "                 images_path: Path,\n",
    "                 labels_path: Path,\n",
    "                 seed: int = 59) -> dict[str, list[Path]]:\n",
    "    \"\"\"\n",
    "    Calculates the size of the train, test and validation splits\n",
    "\n",
    "    Args:\n",
    "        train_ratio (float): Ratio of the train split (Between 0 and 1)\n",
    "        test_ratio (float): Ratio of the train split (Between 0 and 1)\n",
    "        val_ratio (float): Ratio of the train split (Between 0 and 1)\n",
    "        images_path (Path): Path to the images\n",
    "        labels_path (Path): Path to the labels\n",
    "        seed (int): Seed for the randomiser\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[Path]]: A dictionary with keys 'train', 'test' and 'val' denoting each split's files' list.\n",
    "    \"\"\"\n",
    "\n",
    "    ratio_sum = train_ratio + test_ratio + val_ratio\n",
    "    if ratio_sum != 1:\n",
    "        raise ValueError(f\"Invalid ratio sum ({ratio_sum}). Sum of train, test and val ratios should be equal to one\")\n",
    "\n",
    "    if train_ratio <= 0:\n",
    "        raise ValueError(\"Train ratio cannot be less than or equal to 0\")\n",
    "    \n",
    "    # All image extensions are .png by default\n",
    "    all_images = []\n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        all_images.extend(list(images_path.glob(f\"*{ext}\")))\n",
    "\n",
    "    # All label extensions are .txt by default\n",
    "    all_labels = list(labels_path.glob(\"*.txt\"))\n",
    "\n",
    "    if len(all_images) != len(all_labels):\n",
    "        raise ValueError(f\"Length of all images ({len(all_images)}) is not equal to the length of all labels ({len(all_labels)}). Please check the dataset for unlabeled images before proceeding.\")\n",
    "\n",
    "    # Initialise randomisation\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Shuffle all images\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    # Calculate the count of train, test and validation groups\n",
    "    n = len(all_images)\n",
    "    train_count = int(n * train_ratio)\n",
    "    test_count = int(n * test_ratio)\n",
    "    val_count = int(n * val_ratio)\n",
    "\n",
    "    # If the sum of all groups isn't equal to all image count add the difference to the train count\n",
    "    difference = len(all_images) - (train_count + test_count + val_count)\n",
    "    if difference != 0:\n",
    "        train_count += difference\n",
    "    \n",
    "    # Get train, test and val images from all images based on their calculated counts\n",
    "    train_images = all_images[:train_count]                         # Images between 0 and train_count\n",
    "    val_images   = all_images[train_count:train_count + val_count]  # Images between train_count and train_count + val_count\n",
    "    test_images  = all_images[train_count + val_count:]             # Images between train_count + val_count and len(all_images) - 1\n",
    "\n",
    "    print(f\"\"\"Split dataset into: \n",
    "    Train: {train_count}\n",
    "    Val: {val_count}\n",
    "    Test: {test_count}\"\"\")\n",
    "\n",
    "    return {\n",
    "        \"train\": train_images,\n",
    "        \"test\": test_images,\n",
    "        \"val\": val_images \n",
    "    }\n",
    "\n",
    "splits = split_images(train_ratio=TRAIN_RATIO,\n",
    "                      test_ratio=TEST_RATIO,\n",
    "                      val_ratio=VAL_RATIO,\n",
    "                      images_path=images_path,\n",
    "                      labels_path=labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb188bb",
   "metadata": {},
   "source": [
    "### Creating the required dataset directory tree\n",
    "\n",
    "In order to proceed, we need to create the aformentioned directory tree of:\n",
    "\n",
    "<pre>\n",
    ".\n",
    "└── dataset/\n",
    "    ├── images/\n",
    "    │   ├── train\n",
    "    │   ├── test\n",
    "    │   └── val\n",
    "    └── labels/\n",
    "        ├── train\n",
    "        ├── test\n",
    "        └── val\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec0bd679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'dataset\\images\\train' already exists, skipping creation\n",
      "Directory 'dataset\\images\\test' already exists, skipping creation\n",
      "Directory 'dataset\\images\\val' already exists, skipping creation\n",
      "Directory 'dataset\\labels\\train' already exists, skipping creation\n",
      "Directory 'dataset\\labels\\test' already exists, skipping creation\n",
      "Directory 'dataset\\labels\\val' already exists, skipping creation\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset paths\n",
    "def create_dataset_paths(sub_paths: list[str] = [\"images\", \"labels\"], splits: list[str] = [\"train\", \"test\", \"val\"]) -> None:\n",
    "    \"\"\"\n",
    "    Creates the required dataset paths\n",
    "\n",
    "    Args:\n",
    "        sub_paths (list[str]): Sub paths of the dataset. ['images', 'labels'] by default\n",
    "        splits (list[str]): The names of the splits. ['train', 'test', 'val'] by default\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Set required paths\n",
    "    dataset_path = Path(\"./dataset\")\n",
    "\n",
    "    for path in sub_paths:\n",
    "        for split in splits:\n",
    "            new_dir = dataset_path / path / split\n",
    "            if Path.exists(new_dir):\n",
    "                print(f\"Directory '{new_dir}' already exists, skipping creation\")\n",
    "            else:\n",
    "                new_dir.mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"Created directory: '{new_dir}'\")\n",
    "\n",
    "create_dataset_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69811029",
   "metadata": {},
   "source": [
    "### Move the images and labels into their target directories\n",
    "\n",
    "The last step of splitting the dataset is copying the images and labels to the target directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0749bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train split:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 3426/3426 [01:30<00:00, 37.77files/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test split:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 428/428 [00:01<00:00, 413.44files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing val split:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 428/428 [00:01<00:00, 415.47files/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm   # For the progress bar\n",
    "\n",
    "def copy_dataset_contents(dataset_path: Path, splits: dict[str, list[Path]]) -> None:\n",
    "    \"\"\"\n",
    "    Copies the dataset contents to the target\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path): Path to the root dataset folder\n",
    "        splits (dict[str, list[Path]]): Dictionary containing the dataset splits\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the provided dictionary is in the required format\n",
    "    required_keys = [\"train\", \"test\", \"val\"]\n",
    "    for key in required_keys:\n",
    "        if key not in splits:\n",
    "            raise ValueError(f\"Missing key: {key}\")\n",
    "        \n",
    "    # Copy all files\n",
    "    for split, img_list in splits.items():\n",
    "        print(f\"\\nProcessing {split} split:\")\n",
    "        for img_path in tqdm(img_list, desc=f\"{split}\", unit=\"files\"):\n",
    "\n",
    "            # Get the label using the image path name\n",
    "            lbl_path = labels_path / (img_path.stem + '.txt')\n",
    "\n",
    "            # Create the destination paths\n",
    "            dst_img = dataset_path / \"images\" / split / img_path.name\n",
    "            dst_lbl = dataset_path / \"labels\" / split / lbl_path.name\n",
    "\n",
    "            # Copy the images and labels to the destination paths\n",
    "            shutil.copy2(img_path, dst_img)\n",
    "            shutil.copy2(lbl_path, dst_lbl)\n",
    "\n",
    "copy_dataset_contents(dataset_path=dataset_path,\n",
    "                      splits=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a169a36",
   "metadata": {},
   "source": [
    "### Generating the data.yaml file\n",
    "\n",
    "YOLO models use a data.yaml file to determine the classes with their index values and the dataset location. We need to create one if we want YOLO to be able to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"aircraft\", \"armored_vehicle\", \"helicopter\", \"tank\", \"truck\"]\n",
    "\n",
    "def create_yolo_yaml(classes: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Creates the data.yaml file for YOLO to use\n",
    "\n",
    "    Args:\n",
    "        classes (list[str]): List of class names in order.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    class_dict = {}\n",
    "    for i in range(len(classes)):\n",
    "        class_dict[f\"{i}\"] = classes[i]\n",
    "\n",
    "    with open(\"data.yaml\", \"w\") as file:\n",
    "        file.write(f\"\"\"path: {dataset_path}     # The dataset root\n",
    "\n",
    "train: {dataset_path / \"train\"}     # Path to train\n",
    "test: {dataset_path / \"test\"}       # Path to test\n",
    "val: {dataset_path / \"val\"}         # Path to val\n",
    "\n",
    "names:  # Class names\n",
    "\"\"\")\n",
    "        for item in class_dict:\n",
    "            file.write(f\"\\t{item}: {class_dict[item]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
